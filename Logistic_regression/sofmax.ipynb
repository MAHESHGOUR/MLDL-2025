{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf79f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 1.0986\n",
      "\n",
      "True Labels:      [0 0 1 1 2 2]\n",
      "Predicted Labels: [2 2 2 2 2 2]\n",
      "Predicted Probs:\n",
      " [[0.291 0.095 0.614]\n",
      " [0.277 0.077 0.646]\n",
      " [0.31  0.126 0.564]\n",
      " [0.316 0.139 0.546]\n",
      " [0.269 0.07  0.661]\n",
      " [0.254 0.057 0.689]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample dataset (6 samples, 2 features)\n",
    "X = np.array([\n",
    "    [150, 7],   # Class 0\n",
    "    [170, 6],   # Class 0\n",
    "    [120, 8],   # Class 1\n",
    "    [110, 9],   # Class 1\n",
    "    [180, 5],   # Class 2\n",
    "    [200, 4],   # Class 2\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 1, 1, 2, 2])  # Class labels: 0, 1, 2\n",
    "n_samples, n_features = X.shape\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot(y, num_classes):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "Y = one_hot(y, n_classes)  # Shape: (6, 3)\n",
    "\n",
    "# Initialize weights and bias\n",
    "W = np.zeros((n_classes, n_features))  # Shape: (3, 2)\n",
    "b = np.zeros((n_classes,))            # Shape: (3,)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    z -= np.max(z, axis=1, keepdims=True)  # For numerical stability\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-9), axis=1))\n",
    "\n",
    "# Training loop\n",
    "def train(X, Y, W, b, lr=0.0005, epochs=1):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        # Linear logits\n",
    "        Z = X @ W.T + b  # Shape: (n_samples, n_classes)\n",
    "\n",
    "        # Softmax output\n",
    "        y_pred = softmax(Z)\n",
    "\n",
    "        # Loss\n",
    "        loss = cross_entropy(Y, y_pred)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Gradients\n",
    "        error = y_pred - Y  # Shape: (n_samples, n_classes)\n",
    "        dW = (error.T @ X) / n_samples\n",
    "        db = np.mean(error, axis=0)\n",
    "\n",
    "        # Update weights\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "    return W, b, losses\n",
    "\n",
    "# Train the model\n",
    "W_trained, b_trained, losses = train(X, Y, W, b)\n",
    "\n",
    "# Predict function\n",
    "def predict(X, W, b):\n",
    "    logits = X @ W.T + b\n",
    "    probs = softmax(logits)\n",
    "    return np.argmax(probs, axis=1), probs\n",
    "\n",
    "# Test prediction\n",
    "pred_labels, pred_probs = predict(X, W_trained, b_trained)\n",
    "\n",
    "print(\"\\nTrue Labels:     \", y)\n",
    "print(\"Predicted Labels:\", pred_labels)\n",
    "print(\"Predicted Probs:\\n\", np.round(pred_probs, 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
