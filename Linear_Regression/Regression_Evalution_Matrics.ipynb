{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3c2860",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d6632",
   "metadata": {},
   "source": [
    "# üìä Linear Regression Evaluation Metrics\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Mean Absolute Error (MAE)\n",
    "\n",
    "**Formula:**  \n",
    "$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right| $\n",
    "\n",
    "**Description:**\n",
    "- Measures the average magnitude of the errors in predictions.\n",
    "- Does **not** penalize large errors heavily.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower is better.\n",
    "- Same unit as the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mean Squared Error (MSE)\n",
    "\n",
    "**Formula:**  \n",
    "$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "**Description:**\n",
    "- Penalizes larger errors more than MAE due to squaring.\n",
    "- Sensitive to outliers.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower is better.\n",
    "- Units are squared (e.g., ft¬≤).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Formula:**  \n",
    "$ \\text{RMSE} = \\sqrt{\\text{MSE}} $\n",
    "\n",
    "**Description:**\n",
    "- Square root of MSE.\n",
    "- More interpretable than MSE (in original units).\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower is better.\n",
    "- Same unit as the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "# üìä R¬≤ (R-squared) ‚Äì Coefficient of Determination\n",
    "\n",
    "## üî∑ What is R¬≤?\n",
    "\n",
    "**R¬≤ (R-squared)** is a statistical metric that indicates how well the independent variables explain the variability of the dependent variable in a regression model.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Formula\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}\n",
    "$$ \n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( SS_{\\text{res}} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 \\) ‚Üí Residual Sum of Squares  \n",
    "- \\( SS_{\\text{tot}} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 \\) ‚Üí Total Sum of Squares  \n",
    "- \\( y_i \\): Actual value  \n",
    "- \\( \\hat{y}_i \\): Predicted value  \n",
    "- \\( \\bar{y} \\): Mean of actual values  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Interpretation\n",
    "\n",
    "| R¬≤ Value | Meaning |\n",
    "|----------|---------|\n",
    "| 1.0      | Perfect fit |\n",
    "| 0.9+     | Excellent |\n",
    "| 0.7‚Äì0.9  | Good |\n",
    "| 0.5‚Äì0.7  | Moderate |\n",
    "| < 0.5    | Poor |\n",
    "| < 0      | Worse than mean predictor |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Intuition\n",
    "\n",
    "- **R¬≤ = 1** ‚Üí Model explains all variance.\n",
    "- **R¬≤ = 0** ‚Üí Model explains nothing (equal to mean prediction).\n",
    "- **R¬≤ < 0** ‚Üí Model is worse than simply predicting the mean.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Use Cases\n",
    "\n",
    "- Regression model evaluation\n",
    "- Model comparison\n",
    "- Diagnosing underfitting and overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Limitations\n",
    "\n",
    "1. Doesn‚Äôt imply causation\n",
    "2. Sensitive to outliers\n",
    "3. Can‚Äôt compare across different datasets\n",
    "4. Not suitable for nonlinear models\n",
    "5. Doesn‚Äôt penalize overfitting (high R¬≤ might be misleading)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ûï Adjusted R¬≤\n",
    "\n",
    "### üìä Adjusted R¬≤ ‚Äî Penalizing for Too Many Features\n",
    "\n",
    "Adding more features to a regression model can **artificially inflate** the regular R¬≤ score, even if those features are not truly useful.  \n",
    "To address this, we use **Adjusted R¬≤**, which penalizes the addition of unnecessary predictors.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò Formula\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( R^2 \\) = Coefficient of determination  \n",
    "- \\( n \\) = Number of samples  \n",
    "- \\( k \\) = Number of predictors (features)\n",
    "\n",
    "---\n",
    "\n",
    "#### üí° Intuition\n",
    "- **R¬≤** always increases (or stays the same) when you add more features.  \n",
    "- **Adjusted R¬≤** increases **only if** the new feature improves model performance more than expected by chance.  \n",
    "- If a feature doesn‚Äôt help, **Adjusted R¬≤ decreases**, signaling possible overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### üí° Intuition\n",
    "- **R¬≤** always increases (or stays the same) when you add more features.  \n",
    "- **Adjusted R¬≤** increases **only if** the new feature improves model performance more than expected by chance.  \n",
    "- If a feature doesn‚Äôt help, **Adjusted R¬≤ decreases**, signaling possible overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### üßÆ Example in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399a2fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R¬≤ Score: 0.9265\n",
      "Adjusted R¬≤: 0.8529\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Example data\n",
    "y = np.array([10, 12, 15, 18, 20])\n",
    "y_pred = np.array([11, 11, 14, 19, 21])\n",
    "\n",
    "# Compute R¬≤\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# Compute Adjusted R¬≤\n",
    "n = len(y)   # number of samples\n",
    "k = 2        # number of predictors (example)\n",
    "adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - k - 1)\n",
    "\n",
    "print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "print(f\"Adjusted R¬≤: {adj_r2:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f0dd1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Metric         | Penalizes Large Errors | Units          | Ideal Value |\n",
    "|----------------|------------------------|----------------|--------------|\n",
    "| **MAE**        | No                     | Same as target | 0            |\n",
    "| **MSE**        | Yes                    | Squared units  | 0            |\n",
    "| **RMSE**       | Yes                    | Same as target | 0            |\n",
    "| **R¬≤ Score**   | No                     | Dimensionless  | 1            |\n",
    "| **Adjusted R¬≤**| No                     | Dimensionless  | Close to 1   |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324e7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197a22de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# y_true = actual values, y_pred = predicted values\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(\u001b[43my_test\u001b[49m, y_pred)\n\u001b[0;32m      9\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n\u001b[0;32m     10\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "## üß™ Example Code (Scikit-learn)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# y_true = actual values, y_pred = predicted values\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R¬≤ Score: {r2:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
